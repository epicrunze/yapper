{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556468b1",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "\n",
    "This python notebook is used to extract and process a dataset from the wikipedia, used to train YAPPER.\n",
    "\n",
    "The flow is as follows:\n",
    "\n",
    "* Extract English-written Wiki pages in science, geography, ecnonomics, and history\n",
    "* Preproccess extracted text to remove artifacts and whitespace\n",
    "* Semantically chunk them to ~ 500 words long chunks of text. Drop all chunks less than 400 words\n",
    "* Generate metadata for each chunk\n",
    "* Ensure there are no near duplicates through embedding similarity\n",
    "* Split to train/eval/test\n",
    "* Generate dataset summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ef43a",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbabf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mwparserfromhell\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from collections import deque\n",
    "import asyncio\n",
    "import inspect\n",
    "import os\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "import semchunk\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9255ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Env vars\n",
    "config = dotenv_values(\"../.env\")\n",
    "config[\"WIKI_API\"] = \"https://en.wikipedia.org/w/api.php\"\n",
    "config[\"WIKI_BATCH_SIZE\"] = 50\n",
    "config[\"WIKI_HEADERS\"] = {\"User-Agent\": \"WikiDatasetBot/0.1 (contact: your_email@example.com)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606b7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duckdb connection\n",
    "# con = ddb.connect(\"/hpc/home/bfa6/work/data/yapper/database.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beca71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini client\n",
    "client = genai.Client(api_key=config[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7288400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semchunk chunker\n",
    "chunker = semchunk.chunkerify(lambda text: len(text.split()), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43202dc7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e2ba8",
   "metadata": {},
   "source": [
    "### Wiki related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1733fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) utility: ensure category prefix\n",
    "def ensure_cat_prefix(cat_name: str) -> str:\n",
    "    return cat_name if cat_name.startswith(\"Category:\") else \"Category:\" + cat_name\n",
    "\n",
    "# 2) generator: category members (pages / subcats / files) - top-level generator function\n",
    "def category_members_generator(cat_title: str, cmtype: str = \"page\", cmnamespace: int = 0, limit: int = 500, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Yield category members for a category (page dicts from API).\n",
    "    cmtype: \"page\" | \"subcat\" | \"file\"\n",
    "    cmnamespace: namespace filter (0 for articles, 14 for categories)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": cat_title,\n",
    "        \"cmtype\": cmtype,\n",
    "        \"cmnamespace\": cmnamespace,\n",
    "        \"cmlimit\": str(limit),\n",
    "        \"formatversion\": 2\n",
    "    }\n",
    "    cont = {}\n",
    "    while True:\n",
    "        p = params.copy()\n",
    "        p.update(cont)\n",
    "        r = requests.get(config[\"WIKI_API\"], params=p, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        members = j.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for m in members:\n",
    "            yield m\n",
    "        if \"continue\" in j:\n",
    "            cont = j[\"continue\"]\n",
    "        else:\n",
    "            break\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "\n",
    "# 3) gather titles from one seed category (BFS over subcats optional)\n",
    "def gather_titles_from_category(seed_category: str, recurse: bool = False, max_depth: int = 1, max_pages_per_category: int = 1000, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Return a set of page titles (namespace 0) under seed_category.\n",
    "    \"\"\"\n",
    "    seed_cat = ensure_cat_prefix(seed_category)\n",
    "    queue = deque()\n",
    "    queue.append((seed_cat, 0))\n",
    "    seen_cats = {seed_cat}\n",
    "    titles = set()\n",
    "\n",
    "    while queue:\n",
    "        cat, depth = queue.popleft()\n",
    "        count = 0\n",
    "        for m in category_members_generator(cat, cmtype=\"page\", cmnamespace=0, limit=500, delay=delay):\n",
    "            title = m.get(\"title\")\n",
    "            if title:\n",
    "                titles.add(title)\n",
    "            count += 1\n",
    "            if count >= max_pages_per_category:\n",
    "                break\n",
    "\n",
    "        if not recurse or depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        for sub in category_members_generator(cat, cmtype=\"subcat\", cmnamespace=14, limit=500, delay=delay):\n",
    "            sub_title = sub.get(\"title\")\n",
    "            if sub_title and sub_title not in seen_cats:\n",
    "                seen_cats.add(sub_title)\n",
    "                queue.append((sub_title, depth + 1))\n",
    "\n",
    "    return titles\n",
    "\n",
    "# 4) fetch metadata for a list of pageids (batched)\n",
    "def fetch_pages_metadata_by_ids(pageids, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Return list of metadata dicts for the given pageids.\n",
    "    Each dict contains pageid, title, ns, fullurl, length, lastrevid, categories (list).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(pageids), config[\"WIKI_BATCH_SIZE\"]), desc=\"Fetching metadata\", leave=False):\n",
    "        chunk = pageids[i:i+config[\"WIKI_BATCH_SIZE\"]]\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": \"|\".join(str(x) for x in chunk),\n",
    "            \"prop\": \"info|categories\",\n",
    "            \"inprop\": \"url\",\n",
    "            \"cllimit\": \"max\",\n",
    "            \"formatversion\": 2\n",
    "        }\n",
    "        r = requests.get(config[\"WIKI_API\"], params=params, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        pages = j.get(\"query\", {}).get(\"pages\", [])\n",
    "        for p in pages:\n",
    "            rec = {\n",
    "                \"pageid\": p.get(\"pageid\"),\n",
    "                \"title\": p.get(\"title\"),\n",
    "                \"ns\": p.get(\"ns\"),\n",
    "                \"fullurl\": p.get(\"fullurl\"),\n",
    "                \"length\": p.get(\"length\"),\n",
    "                \"lastrevid\": p.get(\"lastrevid\"),\n",
    "                \"categories\": [c.get(\"title\") for c in (p.get(\"categories\") or [])]\n",
    "            }\n",
    "            results.append(rec)\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "    return results\n",
    "\n",
    "# 5) main orchestrator (no nested functions)\n",
    "def get_pages_from_categories(seed_categories, recurse: bool = False, max_depth: int = 1, max_pages_per_seed: int = 1000, delay_between_api_calls: float = 0.0):\n",
    "    \"\"\"\n",
    "    Given a list of seed category names, return a list of enriched page metadata dicts.\n",
    "    - seed_categories: list of strings (with or w/o \"Category:\" prefix)\n",
    "    - returns list of dicts with page metadata and seed_category field.\n",
    "    \"\"\"\n",
    "    # map pageid -> seed_category (first seed encountered)\n",
    "    pageid_to_seed = {}\n",
    "    for seed in tqdm(seed_categories, desc=\"Seed categories\"):\n",
    "        seed_pref = ensure_cat_prefix(seed)\n",
    "        titles = gather_titles_from_category(seed_pref, recurse=recurse, max_depth=max_depth, max_pages_per_category=max_pages_per_seed, delay=delay_between_api_calls)\n",
    "        # gather pageids for these titles using a query (batch)\n",
    "        # We need to convert titles -> pageids in batches\n",
    "        titles_list = list(titles)\n",
    "        for i in tqdm(range(0, len(titles_list), config[\"WIKI_BATCH_SIZE\"]), desc=f\"Converting titles for {seed_pref}\", leave=False):\n",
    "            chunk = titles_list[i:i+config[\"WIKI_BATCH_SIZE\"]]\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": \"|\".join(chunk),\n",
    "                \"prop\": \"info\",\n",
    "                \"inprop\": \"url\",\n",
    "                \"formatversion\": 2\n",
    "            }\n",
    "            r = requests.get(config[\"WIKI_API\"], params=params, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "            r.raise_for_status()\n",
    "            j = r.json()\n",
    "            pages = j.get(\"query\", {}).get(\"pages\", [])\n",
    "            for p in pages:\n",
    "                pid = p.get(\"pageid\")\n",
    "                if pid and pid not in pageid_to_seed:\n",
    "                    pageid_to_seed[pid] = {\"seed_category\": seed_pref}\n",
    "            if delay_between_api_calls:\n",
    "                time.sleep(delay_between_api_calls)\n",
    "\n",
    "    if not pageid_to_seed:\n",
    "        return []\n",
    "\n",
    "    # fetch metadata for collected pageids\n",
    "    pageids = list(pageid_to_seed.keys())\n",
    "    meta_list = fetch_pages_metadata_by_ids(pageids, delay=delay_between_api_calls)\n",
    "\n",
    "    # attach seed_category\n",
    "    for m in meta_list:\n",
    "        pid = m.get(\"pageid\")\n",
    "        m[\"seed_category\"] = pageid_to_seed.get(pid, {}).get(\"seed_category\")\n",
    "\n",
    "    # sort deterministically\n",
    "    meta_list.sort(key=lambda x: (x.get(\"seed_category\") or \"\", x.get(\"title\") or \"\"))\n",
    "    return meta_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65581b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_wikitext(wikitext: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert wikitext to plain text paragraphs only.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove comments\n",
    "    text = re.sub(r'<!--.*?-->', '', wikitext, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove all references (including nested ones)\n",
    "    while '<ref' in text:\n",
    "        text = re.sub(r'<ref[^>]*>.*?</ref>', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'<ref[^>]*\\/>', '', text)\n",
    "    \n",
    "    # Remove infoboxes and templates aggressively\n",
    "    def remove_nested_braces(s):\n",
    "        while '{{' in s:\n",
    "            start = s.find('{{')\n",
    "            if start == -1:\n",
    "                break\n",
    "            count = 0\n",
    "            i = start\n",
    "            while i < len(s):\n",
    "                if s[i:i+2] == '{{':\n",
    "                    count += 1\n",
    "                    i += 2\n",
    "                elif s[i:i+2] == '}}':\n",
    "                    count -= 1\n",
    "                    i += 2\n",
    "                    if count == 0:\n",
    "                        s = s[:start] + s[i:]\n",
    "                        break\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "        return s\n",
    "    \n",
    "    text = remove_nested_braces(text)\n",
    "    \n",
    "    # Remove tables\n",
    "    text = re.sub(r'\\{\\|.*?\\|\\}', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove files/images with nested brackets\n",
    "    def remove_files(s):\n",
    "        while True:\n",
    "            match = re.search(r'\\[\\[(?:File|Image):', s, re.IGNORECASE)\n",
    "            if not match:\n",
    "                break\n",
    "            start = match.start()\n",
    "            count = 0\n",
    "            i = start\n",
    "            while i < len(s):\n",
    "                if s[i:i+2] == '[[':\n",
    "                    count += 1\n",
    "                    i += 2\n",
    "                elif s[i:i+2] == ']]':\n",
    "                    count -= 1\n",
    "                    i += 2\n",
    "                    if count == 0:\n",
    "                        s = s[:start] + s[i:]\n",
    "                        break\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "        return s\n",
    "    \n",
    "    text = remove_files(text)\n",
    "    \n",
    "    # Remove categories\n",
    "    text = re.sub(r'\\[\\[Category:.*?\\]\\]', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Convert wikilinks: [[Link|Display]] -> Display, [[Link]] -> Link\n",
    "    text = re.sub(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]', r'\\1', text)\n",
    "    \n",
    "    # Remove external links\n",
    "    text = re.sub(r'\\[https?://[^\\]]+\\]', '', text)\n",
    "    \n",
    "    # Remove section headers\n",
    "    text = re.sub(r'={2,}.*?={2,}', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove formatting\n",
    "    text = re.sub(r\"'{2,}\", '', text)\n",
    "    \n",
    "    # Filter lines\n",
    "    lines = []\n",
    "    skip_sections = {'Footnotes', 'Bibliography', 'References', 'External links', 'See also', 'Notes', 'Further reading'}\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line or line in skip_sections:\n",
    "            continue\n",
    "        if re.match(r'^[\\*:;\\|!{}\\[\\]]', line):\n",
    "            continue\n",
    "        lines.append(line)\n",
    "    \n",
    "    return '\\n\\n'.join(lines)\n",
    "\n",
    "\n",
    "def get_wikipedia_page_text(title: str, sleep: float = 0.5) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the plain text of a Wikipedia page (English) given its title.\n",
    "\n",
    "    Args:\n",
    "        title (str): The Wikipedia page title (e.g., \"Artificial intelligence\").\n",
    "        sleep (float): Optional pause between requests (in seconds) to be polite.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned plain-text content of the page, or empty string if not found.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"formatversion\": 2,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            config[\"WIKI_API\"],\n",
    "            params=params,\n",
    "            headers=config[\"WIKI_HEADERS\"],\n",
    "            timeout=10\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "        if not pages or \"revisions\" not in pages[0]:\n",
    "            return \"\"\n",
    "\n",
    "        # Raw wikitext\n",
    "        wikitext = pages[0][\"revisions\"][0][\"content\"]\n",
    "        text = _clean_wikitext(wikitext)\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        return text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {title}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7133b",
   "metadata": {},
   "source": [
    "### Duckdb Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f32c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset table\n",
    "def create_tables() -> bool:\n",
    "    ddl = \"\"\"\n",
    "    CREATE OR REPLACE TABLE wiki_pages (\n",
    "        id UUID DEFAULT uuid() PRIMARY KEY,\n",
    "        wiki_page_id INTEGER UNIQUE,\n",
    "        title TEXT,\n",
    "        url TEXT,\n",
    "        categories JSON  -- store list of strings as JSON\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE wiki_chunks (\n",
    "        id UUID DEFAULT uuid() PRIMARY KEY,\n",
    "        chunk TEXT,\n",
    "        questions JSON,  -- store list of strings as JSON\n",
    "        hash TEXT UNIQUE,\n",
    "        page_id UUID \n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        con.execute(ddl)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create tables: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_wiki_pages(pages: list[dict]) -> bool:\n",
    "    if not pages:\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        data = [\n",
    "            (p[\"wiki_page_id\"], p[\"title\"], p[\"url\"], json.dumps(p.get(\"categories\", [])))\n",
    "            for p in pages\n",
    "        ]\n",
    "        con.executemany(\n",
    "            \"INSERT INTO wiki_pages (wiki_page_id, title, url, categories) VALUES (?, ?, ?, ?)\",\n",
    "            data\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Batch insert into wiki_pages failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_wiki_chunks(chunks: list[dict]) -> bool:\n",
    "    if not chunks:\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        data = [\n",
    "            (c[\"chunk\"], json.dumps(c.get(\"questions\", [])), c[\"hash\"], c[\"page_id\"])\n",
    "            for c in chunks\n",
    "        ]\n",
    "        con.executemany(\n",
    "            \"INSERT INTO wiki_chunks (chunk, questions, hash, page_id) VALUES (?, ?, ?, ?)\",\n",
    "            data\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Batch insert into wiki_chunks failed: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4429fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_wiki_pages():\n",
    "    query = \"SELECT * from wiki_pages\"\n",
    "\n",
    "    df_result = con.execute(query).df()\n",
    "\n",
    "    return df_result.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa1a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_stats():\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        (SELECT COUNT(DISTINCT id) FROM wiki_pages) AS num_pages,\n",
    "        (SELECT COUNT(DISTINCT id) FROM wiki_chunks) AS num_chunks,\n",
    "\n",
    "        -- Chunk word counts\n",
    "        (SELECT MIN(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS min_chunk_length,\n",
    "\n",
    "        (SELECT MAX(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS max_chunk_length,\n",
    "\n",
    "        (SELECT AVG(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1)::numeric)\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS avg_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.5)  WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS median_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.25) WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS q1_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.75) WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS q3_chunk_length,\n",
    "\n",
    "        -- Questions \n",
    "        (SELECT SUM(json_array_length(questions)) FROM wiki_chunks WHERE questions IS NOT NULL) AS num_questions, \n",
    "        (SELECT AVG(json_array_length(questions)::numeric) FROM wiki_chunks WHERE questions IS NOT NULL) AS avg_num_questions_per_chunk ;\n",
    "    ;\n",
    "    \"\"\"\n",
    "    df_result = con.execute(query).df()\n",
    "    \n",
    "    return df_result.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c3f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tables(dir_path: str):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    con.execute(\"SELECT * FROM wiki_pages\").df().to_csv(os.path.join(dir_path, \"wiki_pages.csv\"), index=False)\n",
    "    con.execute(\"SELECT * FROM wiki_chunks\").df().to_csv(os.path.join(dir_path, \"wiki_chunks.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7d359",
   "metadata": {},
   "source": [
    "### Gemini Question generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8532cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(\n",
    "    contents: str,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Gemini with retry and standard exception handling.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=contents,\n",
    "            )\n",
    "            return response.text.strip()\n",
    "\n",
    "        except (ConnectionError, TimeoutError) as e:\n",
    "            print(f\"Network issue on attempt {attempt}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {e}\")\n",
    "\n",
    "        # Retry if not the last attempt\n",
    "        if attempt < retries:\n",
    "            sleep_time = backoff_factor ** (attempt - 1)\n",
    "            print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            print(\"All retries failed.\")\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    return \"Failed to generate response after multiple attempts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb49b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(\n",
    "    context: str,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate Q&A pairs from a given context using Gemini,\n",
    "    returning a JSON object like:\n",
    "      {\"QAs\": [{\"Question\": \"...\", \"Answer\": \"...\"}]}\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an expert question generator. \"\n",
    "        \"Given the following text, generate several question–answer pairs \"\n",
    "        \"that test understanding of its key ideas. \"\n",
    "        \"Answers must be short and concise — ideally one short phrase or sentence, not long explanations. \"\n",
    "        \"Respond ONLY in valid JSON with this structure:\\n\"\n",
    "        '{\"QAs\": [{\"Question\": \"<string>\", \"Answer\": \"<string>\"}]}'\n",
    "    )\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=[system_prompt, context],\n",
    "                config={\"response_mime_type\": \"application/json\"},\n",
    "            )\n",
    "\n",
    "            # If response is valid JSON, .parsed gives a Python dict\n",
    "            # print(response.text)\n",
    "            return json.loads(response.text)\n",
    "\n",
    "        except (ConnectionError, TimeoutError) as e:\n",
    "            print(f\"Network issue on attempt {attempt}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {e}\")\n",
    "\n",
    "        if attempt < retries:\n",
    "            sleep_time = backoff_factor ** (attempt - 1)\n",
    "            print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            print(\"All retries failed.\")\n",
    "            return {\"QAs\": []}\n",
    "\n",
    "    return {\"QAs\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f77496",
   "metadata": {},
   "source": [
    "### Semantic Chunking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc45c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(content: str, threshold: int = 400) -> list[str]:\n",
    "    chunks = chunker(content)\n",
    "    valid_chunks = [chunk for chunk in chunks if len(chunk.split()) > threshold]\n",
    "    \n",
    "    return valid_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb2cd5",
   "metadata": {},
   "source": [
    "### Miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1897c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_text(text: str):\n",
    "    text_bytes = text.encode('utf-8')\n",
    "    hash_object = hashlib.sha256(text_bytes)\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    \n",
    "    return hash_hex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcdcd0",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2a9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_articles(\n",
    "    seed_categories: list, \n",
    "    recurse: bool = False, \n",
    "    max_depth: int = 0, \n",
    "    max_pages_per_seed: int = 10000, \n",
    "    delay_between_api_calls: float = 0.01\n",
    "):\n",
    "    # Get articles\n",
    "    pages = get_pages_from_categories(\n",
    "        seed_categories=seed_categories,\n",
    "        recurse=recurse,\n",
    "        max_depth=max_depth, \n",
    "        max_pages_per_seed=max_pages_per_seed, \n",
    "        delay_between_api_calls=delay_between_api_calls\n",
    "    )\n",
    "\n",
    "    # Save to wiki_pages\n",
    "    wiki_pages = [{\"wiki_page_id\": page[\"pageid\"], \"title\": page[\"title\"], \"url\": page[\"fullurl\"], \"categories\": page[\"categories\"]} for page in pages]\n",
    "    result = insert_wiki_pages(pages=wiki_pages)\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe02748",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_wiki_chunks(model: str = \"gemini-2.5-flash\", concurrency: int = 8, chunk_min_threshold: int = 400):\n",
    "    # Get list of wiki page titles and ids\n",
    "    pages = retrieve_wiki_pages()\n",
    "\n",
    "    records = []\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    # Loop (async) to generate all the chunks\n",
    "    for page in tqdm(pages, desc=\"Generating chunks\"):\n",
    "        # Get content\n",
    "        content = get_wikipedia_page_text(page[\"title\"])\n",
    "\n",
    "        # get chunks\n",
    "        chunks = get_chunks(content, threshold=chunk_min_threshold)\n",
    "\n",
    "        # for each chunk, generate Q&As then store the chunk into the database\n",
    "        async def process_chunk(chunk):\n",
    "            async with sem:\n",
    "                qa = generate_questions(context=chunk, model=model)\n",
    "                return {\n",
    "                    \"chunk\": chunk,\n",
    "                    \"questions\": qa[\"QAs\"],\n",
    "                    \"hash\": hash_text(chunk),\n",
    "                    \"page_id\": page[\"id\"]\n",
    "                }\n",
    "\n",
    "        tasks = [asyncio.create_task(process_chunk(chunk)) for chunk in chunks]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        records.extend(results)\n",
    "\n",
    "    # Store all chunks into the database\n",
    "    insert_wiki_chunks(records)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75067a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_dataset(\n",
    "    seed_categories: list, \n",
    "    recurse: bool = False, \n",
    "    max_depth: int = 0, \n",
    "    max_pages_per_seed: int = 10000, \n",
    "    delay_between_api_calls: float = 0.01,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    refresh_tables: bool = True,\n",
    "    page_processing_concurrency: int = 8,\n",
    "    chunk_min_threshold: int = 400\n",
    "):\n",
    "    # Clear tables if refresh_tables=True\n",
    "    if refresh_tables:\n",
    "        flag = create_tables()\n",
    "\n",
    "        if not flag:\n",
    "            raise ValueError(\"Failed to create tables\")\n",
    "\n",
    "    # Find the articles\n",
    "    flag = fetch_all_articles(\n",
    "        seed_categories=seed_categories,\n",
    "        recurse=recurse,\n",
    "        max_depth=max_depth, \n",
    "        max_pages_per_seed=max_pages_per_seed, \n",
    "        delay_between_api_calls=delay_between_api_calls\n",
    "    )\n",
    "\n",
    "    if not flag:\n",
    "        raise ValueError(\"Failed to fetch articles\")\n",
    "\n",
    "    # For each article: Chunk, then save into the chunk table\n",
    "    flag = await generate_wiki_chunks(model=model, concurrency=page_processing_concurrency, chunk_min_threshold=chunk_min_threshold)\n",
    "\n",
    "    if not flag:\n",
    "        raise ValueError(\"Failed to generate wiki chunks\")\n",
    "\n",
    "    # Print some dataset statistics\n",
    "    stats = generate_data_stats()[0]\n",
    "\n",
    "    print(\"Number of pages:\", stats['num_pages'])\n",
    "    print(\"Number of chunks:\", stats['num_chunks'])\n",
    "    print(\"Min chunk length (words):\", stats['min_chunk_length'])\n",
    "    print(\"Max chunk length (words):\", stats['max_chunk_length'])\n",
    "    print(\"Avg chunk length (words):\", stats['avg_chunk_length'])\n",
    "    print(\"Median chunk length (words):\", stats['median_chunk_length'])\n",
    "    print(\"Q1 (25th percentile):\", stats['q1_chunk_length'])\n",
    "    print(\"Q3 (75th percentile):\", stats['q3_chunk_length'])\n",
    "    print(\"Total questions:\", stats['num_questions'])\n",
    "    print(\"Avg questions per chunk:\", stats['avg_num_questions_per_chunk'])\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94b2d9",
   "metadata": {},
   "source": [
    "## Wiki generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e892ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"Science\", \"Geography\", \"Economics\", \"History\"]\n",
    "await generate_dataset(\n",
    "    seed_categories=seeds, \n",
    "    recurse=True, \n",
    "    max_depth=2, \n",
    "    max_pages_per_seed=2000, \n",
    "    delay_between_api_calls=0.01,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    refresh_tables=True,\n",
    "    page_processing_concurrency=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tables(dir_path=\"/hpc/home/bfa6/work/data/yapper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d6cb9",
   "metadata": {},
   "source": [
    "# Moby Dick Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf04b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, os, time\n",
    "from slugify import slugify\n",
    "\n",
    "OUT_DIR = \"/hpc/home/bfa6/work/github/yapper/dataset\"\n",
    "TARGET = 200\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean(t):\n",
    "    t = t.replace(\"\\r\\n\",\"\\n\")\n",
    "    m = re.search(r\"(?mi)\\*\\*\\* *START OF (THIS|THE) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", t)\n",
    "    if m: t = t[m.end():]\n",
    "    m2 = re.search(r\"(?mi)^start of (the|this) project gutenberg\", t)\n",
    "    if m2: t = t[m2.end():]\n",
    "    m3 = re.search(r\"(?mi)^\\s*contents\\s*$\", t[:4000])\n",
    "    chap = re.compile(r\"(?mi)^(chapter|chap\\.|book|part)\\b.*\", re.M)\n",
    "    if m3:\n",
    "        mc = chap.search(t[m3.end():])\n",
    "        if mc: t = t[m3.end()+mc.start():]\n",
    "    m4 = chap.search(t[:10000])\n",
    "    if m4: t = t[m4.start():]\n",
    "    first = t.split(\"\\n\\n\",1)\n",
    "    if len(first)==2 and re.search(r\"(?i)project gutenberg|license|copyright\", first[0]):\n",
    "        t = first[1]\n",
    "    return t.strip()\n",
    "\n",
    "def pick(fmt):\n",
    "    for k,v in fmt.items():\n",
    "        if \"text/plain\" in k.lower(): return v\n",
    "    return None\n",
    "\n",
    "def download_books(n=TARGET):\n",
    "    s = requests.Session()\n",
    "    got = 0\n",
    "    seen = set()\n",
    "    saved = []\n",
    "    url = \"https://gutendex.com/books?languages=en&mime_type=text%2Fplain&sort=descending\"\n",
    "    while url and got < n:\n",
    "        try:\n",
    "            j = s.get(url, timeout=20).json()\n",
    "        except Exception as e:\n",
    "            print(\"page error:\", e); break\n",
    "        page = sorted(j.get(\"results\",[]), key=lambda b: (b.get(\"download_count\") or 0))\n",
    "        for b in page:\n",
    "            if got >= n: break\n",
    "            bid = b.get(\"id\")\n",
    "            if not bid or bid in seen: \n",
    "                continue\n",
    "            u = pick(b.get(\"formats\",{}))\n",
    "            if not u: \n",
    "                seen.add(bid); continue\n",
    "            try:\n",
    "                r = s.get(u, timeout=20)\n",
    "                r.encoding = r.apparent_encoding or \"utf-8\"\n",
    "                txt = clean(r.text)\n",
    "                fn = os.path.join(OUT_DIR, slugify(f\"{bid}-{b.get('title','untitled')}\")[:150]+\".txt\")\n",
    "                # avoid overwriting if file exists (very unlikely because of slug with id)\n",
    "                if not os.path.exists(fn):\n",
    "                    with open(fn,\"w\",encoding=\"utf8\") as f: f.write(txt)\n",
    "                    print(f\"[{got+1}] {b.get('title')} (downloads: {b.get('download_count')})\")\n",
    "                    saved.append(fn); got += 1\n",
    "                else:\n",
    "                    print(\"exists, skip:\", fn)\n",
    "                seen.add(bid)\n",
    "                time.sleep(0.25)\n",
    "            except Exception as e:\n",
    "                print(\"skip\", bid, e)\n",
    "                seen.add(bid)\n",
    "        url = j.get(\"next\")\n",
    "        time.sleep(0.25)\n",
    "    print(f\"Done: saved {got} books to {OUT_DIR}\")\n",
    "    return saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41d856c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] School education, Home Education Series, vol. 3 (of 6) (downloads: 0)\n",
      "[2] The passing of the phantoms : $b A study of evolutionary psychology and morals (downloads: 0)\n",
      "[3] Cats and kittens (downloads: 0)\n",
      "[4] Anthropology and modern life (downloads: 0)\n",
      "[5] The girl at Silver Thistle (downloads: 0)\n",
      "[6] Bear and forbear : $b or, The young skipper of lake Ucayga (downloads: 0)\n",
      "[7] The life of Abdel Kader, ex-sultan of the Arabs of Algeria (downloads: 0)\n",
      "[8] The crusades (downloads: 0)\n",
      "[9] Codes (downloads: 0)\n",
      "[10] A diary of the wreck of His Majesty's ship Challenger, on the western coast of South America, in May, 1835 : $b with an account of the subsequent encampment of the officers and crew, during a period of seven weeks, on the south coast of Chili (downloads: 0)\n",
      "[11] The adventures of Harlequin (downloads: 0)\n",
      "[12] Walks and talks of an American farmer in England (Part 2 of 2) : $b In the years 1850-51. (downloads: 0)\n",
      "[13] Walks and talks of an American farmer in England (Part 1 of 2) (downloads: 0)\n",
      "[14] Roman Catholic opposition to Papal Infallibility (downloads: 0)\n",
      "[15] The girl he loved : $b or, Where love abides (downloads: 0)\n",
      "[16] The olive branch, and other stories (downloads: 37)\n",
      "[17] Sophie Kennedy's experience : $b or, The stepmother (downloads: 198)\n",
      "[18] Colour in woven design : $b Being a treatise on the science and technology of textile colouring (woollen, worsted, cotton and silk materials) (downloads: 222)\n",
      "[19] Scepticism and animal faith : $b Introduction to a system of philosophy (downloads: 241)\n",
      "[20] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 31, September 29, 1832 (downloads: 349)\n",
      "[21] A Lent in earnest : $b or, Sober thoughts for solemn days (downloads: 396)\n",
      "[22] The man elephant : $b A book of African fairy tales (downloads: 411)\n",
      "[23] Buddhism & science (downloads: 481)\n",
      "[24] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 29, September 15, 1832 (downloads: 497)\n",
      "[25] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 27, September 1, 1832 (downloads: 500)\n",
      "[26] Monthly supplement of the penny magazine of the Society for the Diffusion of Useful Knowledge, issue 32, September 1 to September 29, 1832 (downloads: 513)\n",
      "[27] On the frontier : $b or, Scenes in the West (downloads: 514)\n",
      "[28] In mid-air (downloads: 647)\n",
      "[29] Mission of fear (downloads: 721)\n",
      "[30] Jack Hall : $b or, The school days of an American boy (downloads: 833)\n",
      "[31] The claims of decorative art (downloads: 1164)\n",
      "[32] The history of witchcraft and demonology (downloads: 1723)\n",
      "[33] The philosophy of Gassendi (downloads: 523)\n",
      "[34] The works of the Rev. Isaac Watts, D. D. in nine volumes (volume 1 of 9) (downloads: 579)\n",
      "[35] The magic ring, Vol. 2 (of 3) (downloads: 591)\n",
      "[36] Through green glasses : $b Andy Merrigan's great discovery, and other Irish tales (downloads: 665)\n",
      "[37] Problems of the Libyan Desert (downloads: 685)\n",
      "[38] Monthly supplement of the penny magazine of the Society for the Diffusion of Useful Knowledge, issue 26, July 31 to August 31, 1832 (downloads: 690)\n",
      "[39] A silver pool (downloads: 741)\n",
      "[40] Ten years' experience in raising carrots and cabbage (downloads: 813)\n",
      "[41] The magic ring, Vol. 3 (of 3) (downloads: 875)\n",
      "[42] Catherine the Great (downloads: 986)\n",
      "[43] The wilderness hunter (downloads: 991)\n",
      "[44] The handsome Brandons (downloads: 1003)\n",
      "[45] The Gaelic State in the past & future : $b or, \"The crown of a nation\" (downloads: 1019)\n",
      "[46] The silver net : $b Poems (downloads: 1132)\n",
      "[47] Sally Cocksure : $b A school story (downloads: 1146)\n",
      "[48] The history and adventures of the renowned Don Quixote (downloads: 1148)\n",
      "[49] The Watsons (downloads: 1291)\n",
      "[50] Here foloweth a lytell treatyse of the beaute of women newly translated out of Frenshe in to Englyshe (downloads: 1321)\n",
      "[51] She who sleeps : $b A romance of New York and the Nile (downloads: 1374)\n",
      "[52] The autobiography of Sir Henry Morton Stanley, G.C.B. (downloads: 1383)\n",
      "[53] A perfect gentleman (downloads: 1486)\n",
      "[54] The pedigree of fascism : $b A popular essay on the Western philosophy of politics (downloads: 1501)\n",
      "[55] Dot ... dot ... dash ... dot—\"Fire!\" (downloads: 1673)\n",
      "[56] The mother (downloads: 1773)\n",
      "[57] The red terror in Russia (downloads: 1832)\n",
      "[58] An ice cream laboratory guide (downloads: 1990)\n",
      "[59] Chronicles of the house of Borgia (downloads: 2023)\n",
      "[60] Occultists & mystics of all ages (downloads: 2117)\n",
      "[61] The magic ring, Vol. 1 (of 3) (downloads: 2118)\n",
      "[62] War paint (downloads: 2137)\n",
      "[63] Drug themes in science fiction (downloads: 2409)\n",
      "[64] A journey to the world under-ground (downloads: 2497)\n",
      "[65] Trial by water (downloads: 0)\n",
      "[66] The Russian road to China (downloads: 0)\n",
      "[67] Chambers's Journal of Popular Literature, Science, and Art, fifth series, no. 156, vol. III, December 25, 1886 (downloads: 586)\n",
      "[68] Gleanings in Europe : $b Italy, vol. 2 of 2 (downloads: 701)\n",
      "[69] The exploration of Tibesti, Erdi, Borkou, and Ennedi in 1912-1917 : $b a mission entrusted to the author by the French Institute (downloads: 704)\n",
      "[70] Saïd the fisherman (downloads: 787)\n",
      "[71] Twenty years in Roumania (downloads: 875)\n",
      "[72] Wings of silver (downloads: 886)\n",
      "[73] Collected writings of Clarence Edwin Flynn, first series : $b 1929 and earlier (downloads: 910)\n",
      "[74] The autobiography of a seaman (volume 1 of 2) (downloads: 987)\n",
      "[75] Paul Harley's dream (downloads: 1012)\n",
      "[76] A sailor boy with Dewey : $b or, Afloat in the Philippines (downloads: 1119)\n",
      "[77] An idea that saved a business (downloads: 1124)\n",
      "[78] Illustrations of taxation (downloads: 1136)\n",
      "[79] Stopping the leak (downloads: 1184)\n",
      "[80] The mid of the maintop (downloads: 1244)\n",
      "[81] Does civilization need religion? : $b A study in the social resources and limitations of religion in modern life (downloads: 1253)\n",
      "[82] Man's supreme inheritance : $b Conscious guidance and control in relation to human evolution in civilization (downloads: 1306)\n",
      "[83] Classics of modern science : $b (Copernicus to Pasteur) (downloads: 1331)\n",
      "[84] William Shakspere and Robert Greene : $b the evidence (downloads: 1352)\n",
      "[85] The wilderness (downloads: 1392)\n",
      "[86] The adventures of Twinkly Eyes the little Black Bear (downloads: 1394)\n",
      "[87] The Russian road to China (downloads: 1467)\n",
      "[88] The defender (downloads: 1516)\n",
      "[89] The barbarous babes : $b Being the memoirs of Molly (downloads: 1549)\n",
      "[90] Wood Cottage : $b or, Sheltered at last (downloads: 1751)\n",
      "[91] Murder in the Gilded Cage (downloads: 1938)\n",
      "[92] The evolution of the oil industry (downloads: 2000)\n",
      "[93] The historians' history of the world in twenty-five volumes, volume 11 : $b France, 843-1715 (downloads: 2021)\n",
      "[94] The run (downloads: 2077)\n",
      "[95] Old world masters in new world collections (downloads: 2396)\n",
      "[96] The dream detective (downloads: 2751)\n",
      "[97] Uncle Tweazy and his quizzical neighbours, vol. 2 : $b a comi-satiric novel (downloads: 622)\n",
      "[98] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 23, August 11, 1832 (downloads: 651)\n",
      "[99] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 22, August 4, 1832 (downloads: 671)\n",
      "[100] Vanilla culture as practiced in the Seychelles Islands (downloads: 677)\n",
      "[101] Monthly supplement of the penny magazine of the Society for the Diffusion of Useful Knowledge, issue 21, June 30 to July 31, 1832 (downloads: 687)\n",
      "[102] A brief summary in plain language of the most important laws concerning women : $b together with a few observations thereon (downloads: 697)\n",
      "[103] The works of the Reverend George Whitefield, M.A., Vol. 5 (of 6) : $b Containing all his sermons and tracts, etc. (downloads: 728)\n",
      "[104] The works of the Reverend George Whitefield, M.A., Vol. 6 (of 6) : $b Containing all his sermons and tracts, etc. (downloads: 736)\n",
      "[105] Le Selve (downloads: 834)\n",
      "[106] Stick to the raft (downloads: 845)\n",
      "[107] Agricultural zoology (downloads: 864)\n",
      "[108] Liberty and the news (downloads: 1089)\n",
      "[109] The garden yard : $b A handbook of intensive farming (downloads: 1127)\n",
      "[110] The porcelain mask : $b A detective story (downloads: 1161)\n",
      "[111] Ten recreational parties (downloads: 1196)\n",
      "[112] With the Indians in France (downloads: 1213)\n",
      "[113] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 24, August 18, 1832 (downloads: 1237)\n",
      "[114] Select works of Porphyry : $b Containing his four books on abstinence from animal food; his treatise on the Homeric cave of the nymphs; and his Auxiliaries to the perception of intelligible natures. With an appendix, explaining the allegory of the wandering of Ulysses (downloads: 1260)\n",
      "[115] Some forgotten Pennsylvania heroines (downloads: 1261)\n",
      "[116] The organisation of thought, educational and scientific (downloads: 1285)\n",
      "[117] Just in time (downloads: 1290)\n",
      "[118] Trading in Scrabbletown (downloads: 1369)\n",
      "[119] A second day in Mary Carrow's school (downloads: 1385)\n",
      "[120] Just a bit too fast (downloads: 1426)\n",
      "[121] A preface to morals (downloads: 1474)\n",
      "[122] Historical sketches of the south (downloads: 1577)\n",
      "[123] The house with the silver door (downloads: 1715)\n",
      "[124] A millionaire at sixteen : $b or, The cruise of the Guardian-Mother (downloads: 1790)\n",
      "[125] The sinister mark (downloads: 1866)\n",
      "[126] Is 5 (downloads: 1889)\n",
      "[127] The story of the sun, moon, and stars (downloads: 2140)\n",
      "[128] The trail of deception (downloads: 2539)\n",
      "[129] Blackwood's Edinburgh Magazine, Vol. 74, No. 455, September, 1853 (downloads: 585)\n",
      "[130] The orphan nieces : $b or, Duty and inclination (downloads: 742)\n",
      "[131] Blackwood's Edinburgh Magazine, Vol. 74, No. 454, August, 1853 (downloads: 812)\n",
      "[132] Freston Tower : $b A tale of the times of Cardinal Wolsey (downloads: 886)\n",
      "[133] Jock's inheritance (downloads: 911)\n",
      "[134] Folk tales of Sind and Guzarat (downloads: 917)\n",
      "[135] The black spaniel, and other stories (downloads: 951)\n",
      "[136] Skeeter Bill comes to town (downloads: 993)\n",
      "[137] Jonah (downloads: 1018)\n",
      "[138] Blackwood's Edinburgh Magazine, Vol. 79, No. 485, March, 1856 (downloads: 1041)\n",
      "[139] At home in Fiji (downloads: 1061)\n",
      "[140] Spar-torpedo instructions for the United States Navy (downloads: 1087)\n",
      "[141] Sally in her fur coat (downloads: 1091)\n",
      "[142] Rocky Fork (downloads: 1104)\n",
      "[143] Wigwam wonder tales (downloads: 1150)\n",
      "[144] Aspects of science (downloads: 1165)\n",
      "[145] Princess Puck (downloads: 1194)\n",
      "[146] Aeolus; or, the future of the flying machine (downloads: 1246)\n",
      "[147] Saved from herself : $b or, On the edge of doom (downloads: 1249)\n",
      "[148] China collecting in America (downloads: 1389)\n",
      "[149] Neesby Court : $b or, Our pretty cousin (downloads: 1421)\n",
      "[150] Little Button Rose (downloads: 1439)\n",
      "[151] The phantom public (downloads: 1505)\n",
      "[152] The art of courtship (downloads: 1588)\n",
      "[153] The mark of Cain (downloads: 1602)\n",
      "[154] Last letters from the living dead man (downloads: 1605)\n",
      "[155] The North-Americans of yesterday : $b a comparative study of North-American Indian life, customs, and products, on the theory of the ethnic unity of the race (downloads: 1629)\n",
      "[156] Episodes before thirty (downloads: 1665)\n",
      "[157] Law-star for an outlaw (downloads: 1787)\n",
      "[158] Moon of madness (downloads: 1847)\n",
      "[159] An independent daughter (downloads: 1940)\n",
      "[160] The poems of Edgar Allan Poe (downloads: 2214)\n",
      "[161] Maugis, ye sorcerer : $b from ye ancient French : a wonderful tale from ye writings of ye mad savant of ye Maison Maugis in ye olde citie of Mouzon, France (downloads: 294)\n",
      "[162] Elementary lathe practice : $b As adapted to the teaching of machine shop practice in technical schools (downloads: 305)\n",
      "[163] Walter Pater (downloads: 356)\n",
      "[164] Yachting wrinkles : $b A practical and historical handbook of valuable information for the racing and cruising yachtsman (downloads: 403)\n",
      "[165] The blind bow-boy (downloads: 404)\n",
      "[166] The narrative of an explorer in tropical South Africa (downloads: 423)\n",
      "[167] A practical guide for making post-mortem examinations : $b and for the study of morbid anatomy, with directions for embalming the dead, and for the preservation of specimens of morbid anatomy (downloads: 444)\n",
      "[168] False face (downloads: 468)\n",
      "[169] How and what to grow in a kitchen garden of one acre (downloads: 476)\n",
      "[170] What to drink : $b The blue book of beverages; recipes and directions for making and serving non-alcoholic drinks for all occasions (downloads: 519)\n",
      "[171] Scottish toasts (downloads: 531)\n",
      "[172] Stubborn people (downloads: 596)\n",
      "[173] Only a clod (downloads: 611)\n",
      "[174] Weeds used in medicine (downloads: 688)\n",
      "[175] The laws of contrast of colour : $b and their application to the arts of painting, decoration of buildings, mosaic work, tapestry and carpet weaving, calico printing, dress, paper staining, printing, military clothing, illumination, landscape, and flower gardening, &c. (downloads: 713)\n",
      "[176] Naval songs and ballads (downloads: 790)\n",
      "[177] Florentine villas (downloads: 806)\n",
      "[178] Dostoevsky (downloads: 910)\n",
      "[179] Silence, and other stories (downloads: 976)\n",
      "[180] The land beyond the mist (downloads: 1161)\n",
      "[181] Chambers's Journal of Popular Literature, Science, and Art, fifth series, no. 155, vol. III, December 18, 1886 (downloads: 1253)\n",
      "[182] Blackwood's Edinburgh Magazine, Vol. 70, No. 432, October, 1851 (downloads: 1415)\n",
      "[183] War letters from the living dead man (downloads: 1422)\n",
      "[184] The corsair; or, the little fairy at the bottom of the sea : $b A new Christmas burlesque and pantomime, founded upon the ballet of \"Le corsair\" (downloads: 1462)\n",
      "[185] The counterfeiters : $b (Les faux-monnayeurs) (downloads: 1472)\n",
      "[186] The ghost of Charlotte Cray, and other stories (downloads: 1474)\n",
      "[187] The Silver Glen : $b A story of the rebellion of 1715 (downloads: 1616)\n",
      "[188] The history of the Norman conquest of England, its causes and its results, Volume 2 (of 6) (downloads: 1934)\n",
      "[189] The works of John Dryden, now first collected in eighteen volumes. Volume 17 (downloads: 2097)\n",
      "[190] Over the straits (downloads: 2620)\n",
      "[191] Nora's twin sister (downloads: 2702)\n",
      "[192] The literature of the Celts (downloads: 3130)\n",
      "[193] The accomplishment ratio : $b A treatment of the inherited determinants of disparity in school product (downloads: 180)\n",
      "[194] Dead-sea fruit, Vol. 3 (of 3) (downloads: 182)\n",
      "[195] Dead-sea fruit, Vol. 2 (of 3) (downloads: 190)\n",
      "[196] Before the Most Holy = $b (Coram Sanctissimo) (downloads: 194)\n",
      "[197] Webster's practical forestry : $b A popular handbook on the rearing and growth of trees for profit or ornament (downloads: 214)\n",
      "[198] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 15, June 30, 1832 (downloads: 218)\n",
      "[199] The penny magazine of the Society for the Diffusion of Useful Knowledge, issue 18, July 14, 1832 (downloads: 234)\n",
      "[200] Race and nationality (downloads: 244)\n",
      "Done: saved 200 books to /hpc/home/bfa6/work/github/yapper/dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/hpc/home/bfa6/work/github/yapper/dataset/77188-school-education-home-education-series-vol-3-of-6.txt',\n",
       " '/hpc/home/bfa6/work/github/yapper/dataset/77186-the-passing-of-the-phantoms-b-a-study-of-evolutionary-psychology-and-morals.txt',\n",
       " '/hpc/home/bfa6/work/github/yapper/dataset/77183-cats-and-kittens.txt',\n",
       " '/hpc/home/bfa6/work/github/yapper/dataset/77181-anthropology-and-modern-life.txt',\n",
       " '/hpc/home/bfa6/work/github/yapper/dataset/77180-the-girl-at-silver-thistle.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = download_books(200)\n",
    "files[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb224f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/hpc/home/bfa6/work/github/yapper/dataset\"\n",
    "files = [base + \"/\" + file for file in os.listdir(base)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71e3c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "        book = f.read() \n",
    "    chunks = get_chunks(book)\n",
    "    all_chunks += [{\"chunk\": chunk, \"source\": file} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8443053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18914\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32349b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base + \"/chunks.json\", \"w\") as f:\n",
    "    json.dump(all_chunks, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9880cccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': '                         THE CAVE OF ELEPHANTA.\\n\\n [Illustration: A view of a cave, with large statues and pillars and two\\n                         people standing inside.]\\n\\nOne of the earliest monuments of India that attracted the notice of\\nEuropeans was the excavation of Elephanta, situated in a beautiful\\nisland of the same name, called by the natives Goripura, or _Mountain\\nCity_. This island is in the bay of Bombay, seven miles from Bombay\\ncastle; it is about six miles in circumference, and composed of two long\\nhills with a narrow valley between them.\\n\\nThe island has taken its familiar name from a colossal statue of an\\nelephant, cut out of a detached mass of blackish rock unconnected with\\nany stratum below. This figure has had another on its back, which the\\nold travellers call a young elephant, but which, as far as we can judge\\nfrom the drawing of what remains of it, has much more probably been a\\ntiger. The head and neck of this elephant dropped off about 1814, owing\\nto a large fissure that ran up through its back. The length of this\\ncolossal figure, from the forehead to the root of the tail, was 13 feet\\n2 inches; and the height at the head 7 feet 4 inches. The remains of\\nthis colossus stand about 250 yards to the right of the usual\\nlanding-place, which is towards the southern part of the island.\\n\\nAfter proceeding up the valley till the two mountains unite, we come to\\na narrow path, after ascending which there is a beautiful prospect of\\nthe northern part of the island, and the opposite shores of Salsette.\\n“Advancing forward and keeping to the left along the bend of the hill,\\nwe gradually mount to an open space, and come suddenly on the grand\\nentrance of a magnificent temple, whose huge massy columns seem to give\\nsupport to the whole mountain which rises above it.\\n\\n“The entrance into this temple, which is entirely hewn out of a stone\\nresembling porphyry, is by a spacious front supported by two massy\\npillars and two pilasters forming three openings, under a thick and\\nsteep rock overhung by brushwood and wild shrubs. The long ranges of\\ncolumns that appear closing in perspective on every side; the flat roof\\nof solid rock that seems to be prevented from falling only by the massy\\npillars, whose capitals are pressed down and flattened as if by the\\nsuperincumbent weight; the darkness that obscures the interior of the\\ntemple, which is dimly lighted only by the entrances; and the gloomy\\nappearance of the gigantic stone figures ranged along the wall, and\\nhewn, like the whole temple, out of the living rock,--joined to the\\nstrange uncertainty that hangs over the history of this place,--carry\\nthe mind back to distant periods, and impress it with that kind of\\nuncertain and religious awe with which the grander works of ages of\\ndarkness are generally contemplated.',\n",
       "  'source': '/hpc/home/bfa6/work/github/yapper/dataset/76874-the-penny-magazine-of-the-society-for-the-diffusion-of-useful-knowledge-issue-15-june-30-1832.txt'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae2a6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_chunks_qa(chunks: list, model: str = \"gemini-2.5-flash\", concurrency: int = 8):\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def process_chunk(chunk):\n",
    "        async with sem:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            qa = await loop.run_in_executor(\n",
    "                None,  # Uses default ThreadPoolExecutor\n",
    "                generate_questions,\n",
    "                chunk[\"chunk\"],\n",
    "                model\n",
    "            )\n",
    "            return {\n",
    "                \"chunk\": chunk[\"chunk\"],\n",
    "                \"QAs\": qa[\"QAs\"],\n",
    "                \"hash\": hash_text(chunk[\"chunk\"]),\n",
    "                \"source\": chunk[\"source\"]\n",
    "            }\n",
    "\n",
    "    tasks = [process_chunk(chunk) for chunk in chunks]\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Generating Q&A\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b83f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_chunks_qa(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42eb5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
