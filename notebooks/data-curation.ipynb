{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556468b1",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "\n",
    "This python notebook is used to extract and process a dataset from the wikipedia, used to train YAPPER.\n",
    "\n",
    "The flow is as follows:\n",
    "\n",
    "* Extract English-written Wiki pages in science, geography, ecnonomics, and history\n",
    "* Preproccess extracted text to remove artifacts and whitespace\n",
    "* Semantically chunk them to ~ 500 words long chunks of text. Drop all chunks less than 400 words\n",
    "* Generate metadata for each chunk\n",
    "* Ensure there are no near duplicates through embedding similarity\n",
    "* Split to train/eval/test\n",
    "* Generate dataset summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ef43a",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbabf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import mwparserfromhell\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb as ddb\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from collections import deque\n",
    "import asyncio\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "import semchunk\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9255ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Env vars\n",
    "config = dotenv_values(\".env\")\n",
    "config[\"WIKI_API\"] = \"https://en.wikipedia.org/w/api.php\"\n",
    "config[\"WIKI_BATCH_SIZE\"] = 50\n",
    "config[\"WIKI_HEADERS\"] = {\"User-Agent\": \"WikiDatasetBot/0.1 (contact: your_email@example.com)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606b7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duckdb connection\n",
    "con = ddb.connect(\"/hpc/home/bfa6/work/data/yapper/database.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beca71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini client\n",
    "client = genai.Client(api_key=config[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7288400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semchunk chunker\n",
    "chunker = semchunk.chunkerify(lambda text: len(text.split()), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43202dc7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e2ba8",
   "metadata": {},
   "source": [
    "### Wiki related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1733fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) utility: ensure category prefix\n",
    "def ensure_cat_prefix(cat_name: str) -> str:\n",
    "    return cat_name if cat_name.startswith(\"Category:\") else \"Category:\" + cat_name\n",
    "\n",
    "# 2) generator: category members (pages / subcats / files) - top-level generator function\n",
    "def category_members_generator(cat_title: str, cmtype: str = \"page\", cmnamespace: int = 0, limit: int = 500, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Yield category members for a category (page dicts from API).\n",
    "    cmtype: \"page\" | \"subcat\" | \"file\"\n",
    "    cmnamespace: namespace filter (0 for articles, 14 for categories)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": cat_title,\n",
    "        \"cmtype\": cmtype,\n",
    "        \"cmnamespace\": cmnamespace,\n",
    "        \"cmlimit\": str(limit),\n",
    "        \"formatversion\": 2\n",
    "    }\n",
    "    cont = {}\n",
    "    while True:\n",
    "        p = params.copy()\n",
    "        p.update(cont)\n",
    "        r = requests.get(config[\"WIKI_API\"], params=p, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        members = j.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for m in members:\n",
    "            yield m\n",
    "        if \"continue\" in j:\n",
    "            cont = j[\"continue\"]\n",
    "        else:\n",
    "            break\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "\n",
    "# 3) gather titles from one seed category (BFS over subcats optional)\n",
    "def gather_titles_from_category(seed_category: str, recurse: bool = False, max_depth: int = 1, max_pages_per_category: int = 1000, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Return a set of page titles (namespace 0) under seed_category.\n",
    "    \"\"\"\n",
    "    seed_cat = ensure_cat_prefix(seed_category)\n",
    "    queue = deque()\n",
    "    queue.append((seed_cat, 0))\n",
    "    seen_cats = {seed_cat}\n",
    "    titles = set()\n",
    "\n",
    "    while queue:\n",
    "        cat, depth = queue.popleft()\n",
    "        count = 0\n",
    "        for m in category_members_generator(cat, cmtype=\"page\", cmnamespace=0, limit=500, delay=delay):\n",
    "            title = m.get(\"title\")\n",
    "            if title:\n",
    "                titles.add(title)\n",
    "            count += 1\n",
    "            if count >= max_pages_per_category:\n",
    "                break\n",
    "\n",
    "        if not recurse or depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        for sub in category_members_generator(cat, cmtype=\"subcat\", cmnamespace=14, limit=500, delay=delay):\n",
    "            sub_title = sub.get(\"title\")\n",
    "            if sub_title and sub_title not in seen_cats:\n",
    "                seen_cats.add(sub_title)\n",
    "                queue.append((sub_title, depth + 1))\n",
    "\n",
    "    return titles\n",
    "\n",
    "# 4) fetch metadata for a list of pageids (batched)\n",
    "def fetch_pages_metadata_by_ids(pageids, delay: float = 0.0):\n",
    "    \"\"\"\n",
    "    Return list of metadata dicts for the given pageids.\n",
    "    Each dict contains pageid, title, ns, fullurl, length, lastrevid, categories (list).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(pageids), config[\"WIKI_BATCH_SIZE\"]), desc=\"Fetching metadata\", leave=False):\n",
    "        chunk = pageids[i:i+config[\"WIKI_BATCH_SIZE\"]]\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": \"|\".join(str(x) for x in chunk),\n",
    "            \"prop\": \"info|categories\",\n",
    "            \"inprop\": \"url\",\n",
    "            \"cllimit\": \"max\",\n",
    "            \"formatversion\": 2\n",
    "        }\n",
    "        r = requests.get(config[\"WIKI_API\"], params=params, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        pages = j.get(\"query\", {}).get(\"pages\", [])\n",
    "        for p in pages:\n",
    "            rec = {\n",
    "                \"pageid\": p.get(\"pageid\"),\n",
    "                \"title\": p.get(\"title\"),\n",
    "                \"ns\": p.get(\"ns\"),\n",
    "                \"fullurl\": p.get(\"fullurl\"),\n",
    "                \"length\": p.get(\"length\"),\n",
    "                \"lastrevid\": p.get(\"lastrevid\"),\n",
    "                \"categories\": [c.get(\"title\") for c in (p.get(\"categories\") or [])]\n",
    "            }\n",
    "            results.append(rec)\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "    return results\n",
    "\n",
    "# 5) main orchestrator (no nested functions)\n",
    "def get_pages_from_categories(seed_categories, recurse: bool = False, max_depth: int = 1, max_pages_per_seed: int = 1000, delay_between_api_calls: float = 0.0):\n",
    "    \"\"\"\n",
    "    Given a list of seed category names, return a list of enriched page metadata dicts.\n",
    "    - seed_categories: list of strings (with or w/o \"Category:\" prefix)\n",
    "    - returns list of dicts with page metadata and seed_category field.\n",
    "    \"\"\"\n",
    "    # map pageid -> seed_category (first seed encountered)\n",
    "    pageid_to_seed = {}\n",
    "    for seed in tqdm(seed_categories, desc=\"Seed categories\"):\n",
    "        seed_pref = ensure_cat_prefix(seed)\n",
    "        titles = gather_titles_from_category(seed_pref, recurse=recurse, max_depth=max_depth, max_pages_per_category=max_pages_per_seed, delay=delay_between_api_calls)\n",
    "        # gather pageids for these titles using a query (batch)\n",
    "        # We need to convert titles -> pageids in batches\n",
    "        titles_list = list(titles)\n",
    "        for i in tqdm(range(0, len(titles_list), config[\"WIKI_BATCH_SIZE\"]), desc=f\"Converting titles for {seed_pref}\", leave=False):\n",
    "            chunk = titles_list[i:i+config[\"WIKI_BATCH_SIZE\"]]\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": \"|\".join(chunk),\n",
    "                \"prop\": \"info\",\n",
    "                \"inprop\": \"url\",\n",
    "                \"formatversion\": 2\n",
    "            }\n",
    "            r = requests.get(config[\"WIKI_API\"], params=params, headers=config[\"WIKI_HEADERS\"], timeout=30)\n",
    "            r.raise_for_status()\n",
    "            j = r.json()\n",
    "            pages = j.get(\"query\", {}).get(\"pages\", [])\n",
    "            for p in pages:\n",
    "                pid = p.get(\"pageid\")\n",
    "                if pid and pid not in pageid_to_seed:\n",
    "                    pageid_to_seed[pid] = {\"seed_category\": seed_pref}\n",
    "            if delay_between_api_calls:\n",
    "                time.sleep(delay_between_api_calls)\n",
    "\n",
    "    if not pageid_to_seed:\n",
    "        return []\n",
    "\n",
    "    # fetch metadata for collected pageids\n",
    "    pageids = list(pageid_to_seed.keys())\n",
    "    meta_list = fetch_pages_metadata_by_ids(pageids, delay=delay_between_api_calls)\n",
    "\n",
    "    # attach seed_category\n",
    "    for m in meta_list:\n",
    "        pid = m.get(\"pageid\")\n",
    "        m[\"seed_category\"] = pageid_to_seed.get(pid, {}).get(\"seed_category\")\n",
    "\n",
    "    # sort deterministically\n",
    "    meta_list.sort(key=lambda x: (x.get(\"seed_category\") or \"\", x.get(\"title\") or \"\"))\n",
    "    return meta_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65581b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_wikitext(wikitext: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert wikitext to plain text paragraphs only.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove comments\n",
    "    text = re.sub(r'<!--.*?-->', '', wikitext, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove all references (including nested ones)\n",
    "    while '<ref' in text:\n",
    "        text = re.sub(r'<ref[^>]*>.*?</ref>', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'<ref[^>]*\\/>', '', text)\n",
    "    \n",
    "    # Remove infoboxes and templates aggressively\n",
    "    def remove_nested_braces(s):\n",
    "        while '{{' in s:\n",
    "            start = s.find('{{')\n",
    "            if start == -1:\n",
    "                break\n",
    "            count = 0\n",
    "            i = start\n",
    "            while i < len(s):\n",
    "                if s[i:i+2] == '{{':\n",
    "                    count += 1\n",
    "                    i += 2\n",
    "                elif s[i:i+2] == '}}':\n",
    "                    count -= 1\n",
    "                    i += 2\n",
    "                    if count == 0:\n",
    "                        s = s[:start] + s[i:]\n",
    "                        break\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "        return s\n",
    "    \n",
    "    text = remove_nested_braces(text)\n",
    "    \n",
    "    # Remove tables\n",
    "    text = re.sub(r'\\{\\|.*?\\|\\}', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove files/images with nested brackets\n",
    "    def remove_files(s):\n",
    "        while True:\n",
    "            match = re.search(r'\\[\\[(?:File|Image):', s, re.IGNORECASE)\n",
    "            if not match:\n",
    "                break\n",
    "            start = match.start()\n",
    "            count = 0\n",
    "            i = start\n",
    "            while i < len(s):\n",
    "                if s[i:i+2] == '[[':\n",
    "                    count += 1\n",
    "                    i += 2\n",
    "                elif s[i:i+2] == ']]':\n",
    "                    count -= 1\n",
    "                    i += 2\n",
    "                    if count == 0:\n",
    "                        s = s[:start] + s[i:]\n",
    "                        break\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "        return s\n",
    "    \n",
    "    text = remove_files(text)\n",
    "    \n",
    "    # Remove categories\n",
    "    text = re.sub(r'\\[\\[Category:.*?\\]\\]', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Convert wikilinks: [[Link|Display]] -> Display, [[Link]] -> Link\n",
    "    text = re.sub(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]', r'\\1', text)\n",
    "    \n",
    "    # Remove external links\n",
    "    text = re.sub(r'\\[https?://[^\\]]+\\]', '', text)\n",
    "    \n",
    "    # Remove section headers\n",
    "    text = re.sub(r'={2,}.*?={2,}', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove formatting\n",
    "    text = re.sub(r\"'{2,}\", '', text)\n",
    "    \n",
    "    # Filter lines\n",
    "    lines = []\n",
    "    skip_sections = {'Footnotes', 'Bibliography', 'References', 'External links', 'See also', 'Notes', 'Further reading'}\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line or line in skip_sections:\n",
    "            continue\n",
    "        if re.match(r'^[\\*:;\\|!{}\\[\\]]', line):\n",
    "            continue\n",
    "        lines.append(line)\n",
    "    \n",
    "    return '\\n\\n'.join(lines)\n",
    "\n",
    "\n",
    "def get_wikipedia_page_text(title: str, sleep: float = 0.5) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the plain text of a Wikipedia page (English) given its title.\n",
    "\n",
    "    Args:\n",
    "        title (str): The Wikipedia page title (e.g., \"Artificial intelligence\").\n",
    "        sleep (float): Optional pause between requests (in seconds) to be polite.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned plain-text content of the page, or empty string if not found.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"formatversion\": 2,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            config[\"WIKI_API\"],\n",
    "            params=params,\n",
    "            headers=config[\"WIKI_HEADERS\"],\n",
    "            timeout=10\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "        if not pages or \"revisions\" not in pages[0]:\n",
    "            return \"\"\n",
    "\n",
    "        # Raw wikitext\n",
    "        wikitext = pages[0][\"revisions\"][0][\"content\"]\n",
    "        text = _clean_wikitext(wikitext)\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        return text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {title}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7133b",
   "metadata": {},
   "source": [
    "### Duckdb Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f32c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset table\n",
    "def create_tables() -> bool:\n",
    "    ddl = \"\"\"\n",
    "    CREATE OR REPLACE TABLE wiki_pages (\n",
    "        id UUID DEFAULT uuid() PRIMARY KEY,\n",
    "        wiki_page_id INTEGER UNIQUE,\n",
    "        title TEXT,\n",
    "        url TEXT,\n",
    "        categories JSON  -- store list of strings as JSON\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE wiki_chunks (\n",
    "        id UUID DEFAULT uuid() PRIMARY KEY,\n",
    "        chunk TEXT,\n",
    "        questions JSON,  -- store list of strings as JSON\n",
    "        hash TEXT UNIQUE,\n",
    "        page_id UUID \n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        con.execute(ddl)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create tables: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_wiki_pages(pages: list[dict]) -> bool:\n",
    "    if not pages:\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        data = [\n",
    "            (p[\"wiki_page_id\"], p[\"title\"], p[\"url\"], json.dumps(p.get(\"categories\", [])))\n",
    "            for p in pages\n",
    "        ]\n",
    "        con.executemany(\n",
    "            \"INSERT INTO wiki_pages (wiki_page_id, title, url, categories) VALUES (?, ?, ?, ?)\",\n",
    "            data\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Batch insert into wiki_pages failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_wiki_chunks(chunks: list[dict]) -> bool:\n",
    "    if not chunks:\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        data = [\n",
    "            (c[\"chunk\"], json.dumps(c.get(\"questions\", [])), c[\"hash\"], c[\"page_id\"])\n",
    "            for c in chunks\n",
    "        ]\n",
    "        con.executemany(\n",
    "            \"INSERT INTO wiki_chunks (chunk, questions, hash, page_id) VALUES (?, ?, ?, ?)\",\n",
    "            data\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Batch insert into wiki_chunks failed: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4429fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_wiki_pages():\n",
    "    query = \"SELECT * from wiki_pages\"\n",
    "\n",
    "    df_result = con.execute(query).df()\n",
    "\n",
    "    return df_result.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fa1a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_stats():\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        (SELECT COUNT(DISTINCT id) FROM wiki_pages) AS num_pages,\n",
    "        (SELECT COUNT(DISTINCT id) FROM wiki_chunks) AS num_chunks,\n",
    "\n",
    "        -- Chunk word counts\n",
    "        (SELECT MIN(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS min_chunk_length,\n",
    "\n",
    "        (SELECT MAX(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS max_chunk_length,\n",
    "\n",
    "        (SELECT AVG(array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1)::numeric)\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS avg_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.5)  WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS median_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.25) WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS q1_chunk_length,\n",
    "\n",
    "        (SELECT percentile_disc(0.75) WITHIN GROUP (ORDER BY array_length(regexp_split_to_array(trim(chunk), '\\\\s+'), 1))\n",
    "         FROM wiki_chunks WHERE chunk IS NOT NULL AND trim(chunk) <> '') AS q3_chunk_length,\n",
    "\n",
    "    ;\n",
    "    \"\"\"\n",
    "    df_result = con.execute(query).df()\n",
    "    \n",
    "    return df_result.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39c3f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tables(dir_path: str):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    con.execute(\"SELECT * FROM wiki_pages\").df().to_csv(os.path.join(dir_path, \"wiki_pages.csv\"), index=False)\n",
    "    con.execute(\"SELECT * FROM wiki_chunks\").df().to_csv(os.path.join(dir_path, \"wiki_chunks.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7d359",
   "metadata": {},
   "source": [
    "### Gemini Question generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8532cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(\n",
    "    contents: str,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Gemini with retry and standard exception handling.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=contents,\n",
    "            )\n",
    "            return response.text.strip()\n",
    "\n",
    "        except (ConnectionError, TimeoutError) as e:\n",
    "            print(f\"Network issue on attempt {attempt}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {e}\")\n",
    "\n",
    "        # Retry if not the last attempt\n",
    "        if attempt < retries:\n",
    "            sleep_time = backoff_factor ** (attempt - 1)\n",
    "            print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            print(\"All retries failed.\")\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    return \"Failed to generate response after multiple attempts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb49b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(\n",
    "    context: str,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate Q&A pairs from a given context using Gemini,\n",
    "    returning a JSON object like:\n",
    "      {\"QAs\": [{\"Question\": \"...\", \"Answer\": \"...\"}]}\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an expert question generator. \"\n",
    "        \"Given the following text, generate several question–answer pairs \"\n",
    "        \"that test understanding of its key ideas. \"\n",
    "        \"Answers must be short and concise — ideally one short phrase or sentence, not long explanations. \"\n",
    "        \"Respond ONLY in valid JSON with this structure:\\n\"\n",
    "        '{\"QAs\": [{\"Question\": \"<string>\", \"Answer\": \"<string>\"}]}'\n",
    "    )\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=[system_prompt, context],\n",
    "                config={\"response_mime_type\": \"application/json\"},\n",
    "            )\n",
    "\n",
    "            # If response is valid JSON, .parsed gives a Python dict\n",
    "            # print(response.text)\n",
    "            return json.loads(response.text)\n",
    "\n",
    "        except (ConnectionError, TimeoutError) as e:\n",
    "            print(f\"Network issue on attempt {attempt}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {e}\")\n",
    "\n",
    "        if attempt < retries:\n",
    "            sleep_time = backoff_factor ** (attempt - 1)\n",
    "            print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            print(\"All retries failed.\")\n",
    "            return {\"QAs\": []}\n",
    "\n",
    "    return {\"QAs\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f77496",
   "metadata": {},
   "source": [
    "### Semantic Chunking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc45c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(content: str, threshold: int = 400) -> list[str]:\n",
    "    chunks = chunker(content)\n",
    "    valid_chunks = [chunk for chunk in chunks if len(chunk.split()) > threshold]\n",
    "    \n",
    "    return valid_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb2cd5",
   "metadata": {},
   "source": [
    "### Miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1897c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_text(text: str):\n",
    "    text_bytes = text.encode('utf-8')\n",
    "    hash_object = hashlib.sha256(text_bytes)\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    \n",
    "    return hash_hex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcdcd0",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2a9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_articles(\n",
    "    seed_categories: list, \n",
    "    recurse: bool = False, \n",
    "    max_depth: int = 0, \n",
    "    max_pages_per_seed: int = 10000, \n",
    "    delay_between_api_calls: float = 0.01\n",
    "):\n",
    "    # Get articles\n",
    "    pages = get_pages_from_categories(\n",
    "        seed_categories=seed_categories,\n",
    "        recurse=recurse,\n",
    "        max_depth=max_depth, \n",
    "        max_pages_per_seed=max_pages_per_seed, \n",
    "        delay_between_api_calls=delay_between_api_calls\n",
    "    )\n",
    "\n",
    "    # Save to wiki_pages\n",
    "    wiki_pages = [{\"wiki_page_id\": page[\"pageid\"], \"title\": page[\"title\"], \"url\": page[\"fullurl\"], \"categories\": page[\"categories\"]} for page in pages]\n",
    "    result = insert_wiki_pages(pages=wiki_pages)\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbe02748",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_wiki_chunks(model: str = \"gemini-2.5-flash\", concurrency: int = 8, chunk_min_threshold: int = 400):\n",
    "    # Get list of wiki page titles and ids\n",
    "    pages = retrieve_wiki_pages()\n",
    "\n",
    "    records = []\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    # Loop (async) to generate all the chunks\n",
    "    for page in tqdm(pages, desc=\"Generating chunks\"):\n",
    "        # Get content\n",
    "        content = get_wikipedia_page_text(page[\"title\"])\n",
    "\n",
    "        # get chunks\n",
    "        chunks = get_chunks(content, threshold=chunk_min_threshold)\n",
    "\n",
    "        # for each chunk, generate Q&As then store the chunk into the database\n",
    "        async def process_chunk(chunk):\n",
    "            async with sem:\n",
    "                qa = generate_questions(context=chunk, model=model)\n",
    "                return {\n",
    "                    \"chunk\": chunk,\n",
    "                    \"questions\": qa[\"QAs\"],\n",
    "                    \"hash\": hash_text(chunk),\n",
    "                    \"page_id\": page[\"id\"]\n",
    "                }\n",
    "\n",
    "        tasks = [asyncio.create_task(process_chunk(chunk)) for chunk in chunks]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        records.extend(results)\n",
    "\n",
    "    # Store all chunks into the database\n",
    "    insert_wiki_chunks(records)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75067a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_dataset(\n",
    "    seed_categories: list, \n",
    "    recurse: bool = False, \n",
    "    max_depth: int = 0, \n",
    "    max_pages_per_seed: int = 10000, \n",
    "    delay_between_api_calls: float = 0.01,\n",
    "    model: str = \"gemini-2.5-flash\",\n",
    "    refresh_tables: bool = True,\n",
    "    page_processing_concurrency: int = 8,\n",
    "    chunk_min_threshold: int = 400\n",
    "):\n",
    "    # Clear tables if refresh_tables=True\n",
    "    if refresh_tables:\n",
    "        flag = create_tables()\n",
    "\n",
    "        if not flag:\n",
    "            raise ValueError(\"Failed to create tables\")\n",
    "\n",
    "    # Find the articles\n",
    "    flag = fetch_all_articles(\n",
    "        seed_categories=seed_categories,\n",
    "        recurse=recurse,\n",
    "        max_depth=max_depth, \n",
    "        max_pages_per_seed=max_pages_per_seed, \n",
    "        delay_between_api_calls=delay_between_api_calls\n",
    "    )\n",
    "\n",
    "    if not flag:\n",
    "        raise ValueError(\"Failed to fetch articles\")\n",
    "\n",
    "    # For each article: Chunk, then save into the chunk table\n",
    "    flag = await generate_wiki_chunks(model=model, concurrency=page_processing_concurrency, chunk_min_threshold=chunk_min_threshold)\n",
    "\n",
    "    if not flag:\n",
    "        raise ValueError(\"Failed to generate wiki chunks\")\n",
    "\n",
    "    # Print some dataset statistics\n",
    "    stats = generate_data_stats()[0]\n",
    "\n",
    "    print(\"Number of pages:\", stats['num_pages'])\n",
    "    print(\"Number of chunks:\", stats['num_chunks'])\n",
    "    print(\"Min chunk length (words):\", stats['min_chunk_length'])\n",
    "    print(\"Max chunk length (words):\", stats['max_chunk_length'])\n",
    "    print(\"Avg chunk length (words):\", stats['avg_chunk_length'])\n",
    "    print(\"Median chunk length (words):\", stats['median_chunk_length'])\n",
    "    print(\"Q1 (25th percentile):\", stats['q1_chunk_length'])\n",
    "    print(\"Q3 (75th percentile):\", stats['q3_chunk_length'])\n",
    "    print(\"Total questions:\", stats['num_questions'])\n",
    "    print(\"Avg questions per chunk:\", stats['avg_num_questions_per_chunk'])\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94b2d9",
   "metadata": {},
   "source": [
    "## Testing Grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e892ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed categories: 100%|██████████| 4/4 [00:01<00:00,  2.14it/s]\n",
      "Generating chunks: 100%|██████████| 51/51 [12:24<00:00, 14.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 51\n",
      "Number of chunks: 102\n",
      "Min chunk length (words): 401\n",
      "Max chunk length (words): 500\n",
      "Avg chunk length (words): 459.47058823529414\n",
      "Median chunk length (words): 462\n",
      "Q1 (25th percentile): 441\n",
      "Q3 (75th percentile): 478\n",
      "Total questions: 0.0\n",
      "Avg questions per chunk: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = [\"Science\", \"Geography\", \"Economics\", \"History\"]\n",
    "await generate_dataset(\n",
    "    seed_categories=seeds, \n",
    "    recurse=False, \n",
    "    max_depth=1, \n",
    "    max_pages_per_seed=1000, \n",
    "    delay_between_api_calls=0.01,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    refresh_tables=True,\n",
    "    page_processing_concurrency=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb9ad10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tables(dir_path=\"/hpc/home/bfa6/work/data/yapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "191b0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>questions</th>\n",
       "      <th>hash</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bcaf0b72-4500-4348-9042-63765ca52645</td>\n",
       "      <td>An economic impact analysis (EIA) examines the...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What does an Economic I...</td>\n",
       "      <td>80359810820c67b89474865165b1ce38b68c6df0a374ec...</td>\n",
       "      <td>7b736f4f-89cf-43bf-82e0-8337d45a22b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b406cdc2-6c50-4dc6-a5a5-380f2bcd369c</td>\n",
       "      <td>Economics () is a social science that studies ...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What is the primary foc...</td>\n",
       "      <td>e09ee23dd8215fa7f0edb1e75550b6c9049d99ebd53bb7...</td>\n",
       "      <td>c7a27ee4-c1aa-462c-9d84-0cf636a62d4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fea860ac-11da-4b66-b1b5-64e6f9f8cb59</td>\n",
       "      <td>Questions regarding distribution of resources ...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"Who did some economic h...</td>\n",
       "      <td>c6254cd1bcd5f273dc394b6d0927109c49fa8aa8d55735...</td>\n",
       "      <td>c7a27ee4-c1aa-462c-9d84-0cf636a62d4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7878af4e-2ac4-4546-b83f-d4441a2d00a4</td>\n",
       "      <td>Smith discusses potential benefits of speciali...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What did Adam Smith ide...</td>\n",
       "      <td>c84b6660dc8ebe73f2e6612fb8f30f1153f1b1914b41ce...</td>\n",
       "      <td>c7a27ee4-c1aa-462c-9d84-0cf636a62d4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00b87c29-76a3-47b9-9911-24989d2f568c</td>\n",
       "      <td>Value theory was important in classical theory...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What did Adam Smith ide...</td>\n",
       "      <td>3c2a1e21cda9bf54abb8c98e430639e624173b0aae4806...</td>\n",
       "      <td>c7a27ee4-c1aa-462c-9d84-0cf636a62d4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>d750147b-2785-4aba-9da6-7d495a2d8001</td>\n",
       "      <td>There are different schools of thought in the ...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What is the most popula...</td>\n",
       "      <td>7db5488084968b152e0e5b6ebb30ea2ee9e8b55619a203...</td>\n",
       "      <td>3a497910-e7b9-4f2b-987f-1b81d54a143c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>af7d0451-c5ea-4d1c-a23f-f8643bb3a43f</td>\n",
       "      <td>The scientific community is a network of inter...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What is the primary pur...</td>\n",
       "      <td>352088935319a776400233161efb6af9cd346ed061572f...</td>\n",
       "      <td>3a497910-e7b9-4f2b-987f-1b81d54a143c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>8e82fac4-bcf3-49a3-bd55-bed7c9d7c5e0</td>\n",
       "      <td>Funding of science is often through a competit...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What is the typical pro...</td>\n",
       "      <td>c44824973302bf75f6a10974c50ff3c08cc98253766747...</td>\n",
       "      <td>3a497910-e7b9-4f2b-987f-1b81d54a143c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cf353e53-7fd2-4272-89ff-0d07875b2651</td>\n",
       "      <td>Science magazines such as New Scientist, Scien...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"Name two types of media...</td>\n",
       "      <td>b107096835f5143d2670af76243ca5f33bdedfb629ac57...</td>\n",
       "      <td>3a497910-e7b9-4f2b-987f-1b81d54a143c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>7b5d82e1-cb2f-48db-bb32-e41daaa440a8</td>\n",
       "      <td>Scienticide is a concept used to refer to vari...</td>\n",
       "      <td>{\"QAs\": [{\"Question\": \"What is scienticide?\", ...</td>\n",
       "      <td>ef3c145a99903794821ccf2725ea02f76604fd91a4ca6f...</td>\n",
       "      <td>730d6473-8db4-43f1-8de7-4249e40087ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    bcaf0b72-4500-4348-9042-63765ca52645   \n",
       "1    b406cdc2-6c50-4dc6-a5a5-380f2bcd369c   \n",
       "2    fea860ac-11da-4b66-b1b5-64e6f9f8cb59   \n",
       "3    7878af4e-2ac4-4546-b83f-d4441a2d00a4   \n",
       "4    00b87c29-76a3-47b9-9911-24989d2f568c   \n",
       "..                                    ...   \n",
       "97   d750147b-2785-4aba-9da6-7d495a2d8001   \n",
       "98   af7d0451-c5ea-4d1c-a23f-f8643bb3a43f   \n",
       "99   8e82fac4-bcf3-49a3-bd55-bed7c9d7c5e0   \n",
       "100  cf353e53-7fd2-4272-89ff-0d07875b2651   \n",
       "101  7b5d82e1-cb2f-48db-bb32-e41daaa440a8   \n",
       "\n",
       "                                                 chunk  \\\n",
       "0    An economic impact analysis (EIA) examines the...   \n",
       "1    Economics () is a social science that studies ...   \n",
       "2    Questions regarding distribution of resources ...   \n",
       "3    Smith discusses potential benefits of speciali...   \n",
       "4    Value theory was important in classical theory...   \n",
       "..                                                 ...   \n",
       "97   There are different schools of thought in the ...   \n",
       "98   The scientific community is a network of inter...   \n",
       "99   Funding of science is often through a competit...   \n",
       "100  Science magazines such as New Scientist, Scien...   \n",
       "101  Scienticide is a concept used to refer to vari...   \n",
       "\n",
       "                                             questions  \\\n",
       "0    {\"QAs\": [{\"Question\": \"What does an Economic I...   \n",
       "1    {\"QAs\": [{\"Question\": \"What is the primary foc...   \n",
       "2    {\"QAs\": [{\"Question\": \"Who did some economic h...   \n",
       "3    {\"QAs\": [{\"Question\": \"What did Adam Smith ide...   \n",
       "4    {\"QAs\": [{\"Question\": \"What did Adam Smith ide...   \n",
       "..                                                 ...   \n",
       "97   {\"QAs\": [{\"Question\": \"What is the most popula...   \n",
       "98   {\"QAs\": [{\"Question\": \"What is the primary pur...   \n",
       "99   {\"QAs\": [{\"Question\": \"What is the typical pro...   \n",
       "100  {\"QAs\": [{\"Question\": \"Name two types of media...   \n",
       "101  {\"QAs\": [{\"Question\": \"What is scienticide?\", ...   \n",
       "\n",
       "                                                  hash  \\\n",
       "0    80359810820c67b89474865165b1ce38b68c6df0a374ec...   \n",
       "1    e09ee23dd8215fa7f0edb1e75550b6c9049d99ebd53bb7...   \n",
       "2    c6254cd1bcd5f273dc394b6d0927109c49fa8aa8d55735...   \n",
       "3    c84b6660dc8ebe73f2e6612fb8f30f1153f1b1914b41ce...   \n",
       "4    3c2a1e21cda9bf54abb8c98e430639e624173b0aae4806...   \n",
       "..                                                 ...   \n",
       "97   7db5488084968b152e0e5b6ebb30ea2ee9e8b55619a203...   \n",
       "98   352088935319a776400233161efb6af9cd346ed061572f...   \n",
       "99   c44824973302bf75f6a10974c50ff3c08cc98253766747...   \n",
       "100  b107096835f5143d2670af76243ca5f33bdedfb629ac57...   \n",
       "101  ef3c145a99903794821ccf2725ea02f76604fd91a4ca6f...   \n",
       "\n",
       "                                  page_id  \n",
       "0    7b736f4f-89cf-43bf-82e0-8337d45a22b3  \n",
       "1    c7a27ee4-c1aa-462c-9d84-0cf636a62d4c  \n",
       "2    c7a27ee4-c1aa-462c-9d84-0cf636a62d4c  \n",
       "3    c7a27ee4-c1aa-462c-9d84-0cf636a62d4c  \n",
       "4    c7a27ee4-c1aa-462c-9d84-0cf636a62d4c  \n",
       "..                                    ...  \n",
       "97   3a497910-e7b9-4f2b-987f-1b81d54a143c  \n",
       "98   3a497910-e7b9-4f2b-987f-1b81d54a143c  \n",
       "99   3a497910-e7b9-4f2b-987f-1b81d54a143c  \n",
       "100  3a497910-e7b9-4f2b-987f-1b81d54a143c  \n",
       "101  730d6473-8db4-43f1-8de7-4249e40087ca  \n",
       "\n",
       "[102 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM wiki_chunks\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e3f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
