{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc5cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/home/bfa6/work/github/yapper/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:10:17 [__init__.py:216] Automatically detected platform cuda.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/hpc/home/bfa6/work/llms/.cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/hpc/home/bfa6/work/llms/.cache\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from google import genai\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TextStreamer\n",
    "import vllm\n",
    "import nltk\n",
    "\n",
    "config = dotenv_values(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394fc685",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eea8516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:10:35 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\n",
      "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA RTX 5000 Ada Generation. Num GPUs = 1. Max memory: 31.473 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen3-0.6B with actual GPU utilization = 89.01%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 31.47 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 27.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 11-13 18:10:38 [utils.py:233] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.8900533490025423, 'max_num_batched_tokens': 2048, 'max_num_seqs': 320, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":27,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/Qwen3-0.6B'}\n",
      "INFO 11-13 18:10:38 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:10:38 [model.py:1510] Using max model len 2048\n",
      "INFO 11-13 18:10:41 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 11-13 18:10:41 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 11-13 18:10:42 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='unsloth/Qwen3-0.6B', speculative_config=None, tokenizer='unsloth/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":27,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-13 18:10:42 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 11-13 18:10:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 11-13 18:10:43 [gpu_model_runner.py:2602] Starting to load model unsloth/Qwen3-0.6B...\n",
      "INFO 11-13 18:10:43 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 11-13 18:10:44 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "INFO 11-13 18:10:44 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "INFO 11-13 18:10:44 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a706a43b28e44589d6d644541cc94ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:10:46 [default_loader.py:267] Loading weights took 1.99 seconds\n",
      "INFO 11-13 18:10:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 11-13 18:10:47 [gpu_model_runner.py:2653] Model loading took 1.1649 GiB and 3.078017 seconds\n",
      "INFO 11-13 18:10:55 [backends.py:548] Using cache directory: /hpc/home/bfa6/.cache/vllm/torch_compile_cache/3f1f7925d5/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-13 18:10:55 [backends.py:559] Dynamo bytecode transform time: 6.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  4.61it/s, triton_poi_fused_view_6]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:11:03 [backends.py:197] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.37it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 136.36it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 124.98it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 126.76it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 120.81it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 125.33it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 128.57it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 121.99it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 116.16it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 130.63it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 136.76it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 132.69it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 139.07it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 134.75it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 133.68it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 120.31it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 124.10it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 126.02it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 125.15it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 126.43it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 114.24it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 123.70it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 113.03it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 140.24it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 100.33it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 106.90it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 121.83it/s, triton_poi_fused_view_10]                         \n",
      "Unsloth: Compiling kernels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 15.36it/s, triton_per_fused__to_copy_add_mean_mul_pow_rsqrt_4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:11:43 [backends.py:218] Compiling a graph for dynamic shape takes 47.70 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:12:14 [monitor.py:34] torch.compile takes 54.51 s in total\n",
      "INFO 11-13 18:12:16 [gpu_worker.py:298] Available KV cache memory: 25.08 GiB\n",
      "INFO 11-13 18:12:16 [kv_cache_utils.py:1087] GPU KV cache size: 234,784 tokens\n",
      "INFO 11-13 18:12:16 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 114.64x\n",
      "INFO 11-13 18:12:16 [vllm_utils.py:705] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:18<00:00,  3.58it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:03<00:00, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:12:39 [gpu_model_runner.py:3480] Graph capturing finished in 23 secs, took 0.98 GiB\n",
      "INFO 11-13 18:12:39 [vllm_utils.py:712] Unsloth: Patched vLLM v1 graph capture finished in 23 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 18:12:40 [core.py:210] init engine (profile, create kv cache, warmup model) took 113.02 seconds\n",
      "INFO 11-13 18:12:41 [llm.py:306] Supported_tasks: ('generate',)\n",
      "Unsloth: Just some info: will skip parsing ['ffn_norm', 'norm1', 'post_layernorm', 'q_norm', 'attention_norm', 'norm2', 'k_norm', 'input_layernorm', 'post_attention_layernorm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'layer_norm2', 'norm', 'layer_norm1']\n",
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['ffn_norm', 'norm1', 'post_layernorm', 'q_norm', 'attention_norm', 'cross_attn_post_attention_layernorm', 'norm2', 'k_norm', 'input_layernorm', 'post_attention_layernorm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'layer_norm2', 'norm', 'layer_norm1', 'cross_attn_input_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-0.6B\",\n",
    "    cache_dir=\"/hpc/home/bfa6/work/llms/.cache\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    "    force_download=True\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e25791",
   "metadata": {},
   "source": [
    "# Create a chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbf5bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given some context.\\nYour goal is to compress the information in the context and output the compressed version.\\nUse as few tokens as possible while keeping all information.\\nDo NOT produce internal chain-of-thought or step-by-step reasoning.\\nStart immediately with the compressed content (no extra preface).'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are given some context.\\n\"\n",
    "    \"Your goal is to compress the information in the context and output the compressed version.\\n\"\n",
    "    \"Use as few tokens as possible while keeping all information.\\n\"\n",
    "    \"Do NOT produce internal chain-of-thought or step-by-step reasoning.\\n\"\n",
    "    \"Start immediately with the compressed content (no extra preface).\"\n",
    ")\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for forward_message in messages %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set message = messages[index] %}\\n    {%- set current_content = message.content if message.content is defined and message.content is not none else \\'\\' %}\\n    {%- set tool_start = \\'<tool_response>\\' %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = current_content[:tool_start_length] %}\\n    {%- set tool_end = \\'</tool_response>\\' %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (current_content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = current_content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set m_content = message.content if message.content is defined and message.content is not none else \\'\\' %}\\n        {%- set content = m_content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in m_content %}\\n                {%- set content = (m_content.split(\\'</think>\\')|last).lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (m_content.split(\\'</think>\\')|first).rstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and (not reasoning_content.strip() == \\'\\')) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_template = \\\n",
    "#     \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "#         \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "#         \"{% set loop_messages = messages[1:] %}\"\\\n",
    "#     \"{% else %}\"\\\n",
    "#         \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "#         \"{% set loop_messages = messages %}\"\\\n",
    "#     \"{% endif %}\"\\\n",
    "#     \"{% for message in loop_messages %}\"\\\n",
    "#         \"{% if message['role'] == 'user' %}\"\\\n",
    "#             \"{{ '<|user|>\\\\n' + message['content'] + eos_token }}\"\\\n",
    "#         \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "#             \"{{ '<|assistant|>\\\\n' + message['content'] + eos_token }}\"\\\n",
    "#         \"{% endif %}\"\\\n",
    "#     \"{% endfor %}\"\n",
    "\n",
    "# # Replace with out specific template:\n",
    "# chat_template = chat_template\\\n",
    "#     .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\n",
    "\n",
    "# from unsloth.chat_templates import qwen3_template\n",
    "# tokenizer.chat_template = qwen3_template\n",
    "\n",
    "tokenizer.chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0946163",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c453d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "with open(\"/hpc/home/bfa6/work/github/yapper/dataset/chunks.json\", \"r\") as f:\n",
    "    dataset =  json.load(f)\n",
    "\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30422e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15131 1891 1892\n"
     ]
    }
   ],
   "source": [
    "# make sure `dataset` is a list or indexable sequence of length 18_914\n",
    "random.Random(42).shuffle(dataset)  # reproducible shuffle\n",
    "\n",
    "n_total = len(dataset)\n",
    "n_train = 15_131\n",
    "n_eval  = 1_891\n",
    "n_test  = 1_892\n",
    "\n",
    "train_dataset = dataset[:n_train]\n",
    "eval_dataset  = dataset[n_train:n_train + n_eval]\n",
    "test_dataset  = dataset[n_train + n_eval:n_train + n_eval + n_test]\n",
    "\n",
    "print(len(train_dataset), len(eval_dataset), len(test_dataset))\n",
    "# ‚Üí 15131 1891 1892\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6529c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [{\"role\":\"system\", \"content\": system_prompt}, {\"role\":\"user\", \"content\": train_dataset[0][\"chunk\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803aed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk': '‚ÄúIt would not be easy,‚Äù Bill thought, feeling that she did not know\\nmuch about the subjects of greatest interest to the ladies present;\\nbut then, as she soon found, Miss Dawson did not either, and so wisely\\nconfined herself to entertaining the men. Bill did not feel very\\nhopeful of her own powers in that direction, and before she could\\nmake any definite plans her thoughts were interrupted by Mr. Dane‚Äôs\\nentrance into the drawing-room to which everyone had now returned.\\nMr. Dane never joined these parties till after tea, on the excuse of\\nparish-work. After the little disturbance created by his entrance had\\nsubsided, and he had shaken hands with everybody, Bill found that he\\nhad taken the chair next to her. She knew that he wanted to hear if\\nshe had been to Wood Hall, and she was quite ready to tell him. It was\\neasy enough to do this unnoticed in the buzz of general conversation;\\nand accordingly she told him how she and Polly had driven to Wood\\nHall, how Polly had waited outside, and how Mr. Harborough had laid\\nno fresh conditions upon her. This was all very well, but it was not\\nso well when she went on to talk of Mr. Harborough‚Äôs loneliness, and\\nso, incidentally, of her suggestion of a paid companion, and his offer\\nof the post to herself. ‚ÄúOf course he did not mean it really,‚Äù she\\nconcluded; ‚Äúit was only in fun, but for a moment I thought he meant\\nit.‚Äù\\n\\n‚ÄúWhat made you think he did not mean it?‚Äù\\n\\n‚ÄúWhat he said afterwards;‚Äù and she related all that followed. ‚ÄúHe meant\\nhe would have to marry me before they would let me come,‚Äù she said,\\nlaughing a little.\\n\\nBut Mr. Dane did not laugh. ‚ÄúYes, marry him,‚Äù he said, ‚Äúmarry him for\\nWood Hall, for his name and position,--would you do that?‚Äù\\n\\n‚ÄúI did not have the chance; he did not ask me really; it was all fun.‚Äù\\n\\n‚ÄúHave you told your cousins of the fun?‚Äù\\n\\nThe old man was looking earnestly at her, waiting for her answer, and\\nshe hesitated before she gave it. She plainly heard Mrs. Perry saying,\\n‚ÄúI never had a sitting of eggs from the Possets turn out badly,‚Äù before\\nshe said, ‚ÄúNo, I have not told them.‚Äù And she wondered why she had not,\\nand why she never would, for she knew then that she never would.\\n\\n‚ÄúIf he had meant it, would you have taken him and Wood Hall and the\\nname, and the little you know, and the infinitely more which you do not\\nknow?‚Äù',\n",
       " 'source': '/hpc/home/bfa6/work/github/yapper/dataset/76967-princess-puck.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ebe5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mr. Dane did not mean it.\"<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    test,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False     \n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    temperature = 0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0,\n",
    "    max_new_tokens = 2048,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034ee0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Mr. Dane did not mean it.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15362588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚ÄúIt would not be easy,‚Äù Bill thought, feeling that she did not know\\nmuch about the subjects of greatest interest to the ladies present;\\nbut then, as she soon found, Miss Dawson did not either, and so wisely\\nconfined herself to entertaining the men. Bill did not feel very\\nhopeful of her own powers in that direction, and before she could\\nmake any definite plans her thoughts were interrupted by Mr. Dane‚Äôs\\nentrance into the drawing-room to which everyone had now returned.\\nMr. Dane never joined these parties till after tea, on the excuse of\\nparish-work. After the little disturbance created by his entrance had\\nsubsided, and he had shaken hands with everybody, Bill found that he\\nhad taken the chair next to her. She knew that he wanted to hear if\\nshe had been to Wood Hall, and she was quite ready to tell him. It was\\neasy enough to do this unnoticed in the buzz of general conversation;\\nand accordingly she told him how she and Polly had driven to Wood\\nHall, how Polly had waited outside, and how Mr. Harborough had laid\\nno fresh conditions upon her. This was all very well, but it was not\\nso well when she went on to talk of Mr. Harborough‚Äôs loneliness, and\\nso, incidentally, of her suggestion of a paid companion, and his offer\\nof the post to herself. ‚ÄúOf course he did not mean it really,‚Äù she\\nconcluded; ‚Äúit was only in fun, but for a moment I thought he meant\\nit.‚Äù\\n\\n‚ÄúWhat made you think he did not mean it?‚Äù\\n\\n‚ÄúWhat he said afterwards;‚Äù and she related all that followed. ‚ÄúHe meant\\nhe would have to marry me before they would let me come,‚Äù she said,\\nlaughing a little.\\n\\nBut Mr. Dane did not laugh. ‚ÄúYes, marry him,‚Äù he said, ‚Äúmarry him for\\nWood Hall, for his name and position,--would you do that?‚Äù\\n\\n‚ÄúI did not have the chance; he did not ask me really; it was all fun.‚Äù\\n\\n‚ÄúHave you told your cousins of the fun?‚Äù\\n\\nThe old man was looking earnestly at her, waiting for her answer, and\\nshe hesitated before she gave it. She plainly heard Mrs. Perry saying,\\n‚ÄúI never had a sitting of eggs from the Possets turn out badly,‚Äù before\\nshe said, ‚ÄúNo, I have not told them.‚Äù And she wondered why she had not,\\nand why she never would, for she knew then that she never would.\\n\\n‚ÄúIf he had meant it, would you have taken him and Wood Hall and the\\nname, and the little you know, and the infinitely more which you do not\\nknow?‚Äù'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_tokens = tokenizer(train_dataset[0][\"chunk\"])\n",
    "\n",
    "tokenizer.decode(chunk_tokens[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac23a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the input was 556\n",
      "The number of tokens in the output was 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"The number of tokens in the input was {len(chunk_tokens[\"input_ids\"])}\"\"\")\n",
    "print(f\"The number of tokens in the output was {len(output[0])-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7262d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6245dc565b834cd195b873566dd8838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Dataset\n",
    "trainset = [\n",
    "    {\"prompt\": sample[\"chunk\"]} for sample in train_dataset\n",
    "]\n",
    "trainset = Dataset.from_list(trainset)\n",
    "trainset = trainset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772aed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95218c035e8c40c2b83567c6c522cff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eval Dataset\n",
    "evalset = [\n",
    "    {\"prompt\": sample[\"chunk\"]} for sample in eval_dataset\n",
    "]\n",
    "evalset = Dataset.from_list(evalset)\n",
    "evalset = evalset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cb746",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecd387",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47e47921",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_system_prompt = (\n",
    "    \"You are given compressed context created by another model.\\n\"\n",
    "    \"Your goal is to accurately reconstruct the original uncompressed content.\\n\"\n",
    "    \"Expand all abbreviated, shortened, or implied information back to its full form.\\n\"\n",
    "    \"Ensure that no information is lost or altered from the original meaning.\\n\"\n",
    "    \"Do NOT include any reasoning or commentary ‚Äî only output the reconstructed content.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ccb84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recontruct_input(output):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": decoding_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": output}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False     \n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    resp = model.generate(\n",
    "        **inputs,\n",
    "        temperature = 0.7,\n",
    "        top_p=0.8,\n",
    "        top_k=20,\n",
    "        min_p=0.0,\n",
    "        max_new_tokens = 2048,\n",
    "    )\n",
    "\n",
    "    reconstructed_tokens = resp[:, inputs[\"input_ids\"].shape[1]:]\n",
    "\n",
    "    reconstructed_text = tokenizer.decode(reconstructed_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    return reconstructed_text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bdb29",
   "metadata": {},
   "source": [
    "## Reward formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9aa2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text: str):\n",
    "    chunk_tokens = tokenizer(text)\n",
    "    return float(len(chunk_tokens[\"input_ids\"]))\n",
    "\n",
    "\n",
    "\n",
    "def get_bleu_score(original, reconstructed):\n",
    "    hypothesis = reconstructed.split()\n",
    "    reference = original.split()\n",
    "    \n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "    return BLEUscore - 1\n",
    "\n",
    "def get_length_reward(original, compressed):\n",
    "    len_original = get_num_tokens(original)\n",
    "    len_compressed = get_num_tokens(compressed)\n",
    "\n",
    "    r = (len_original - len_compressed) * (1/len_original)\n",
    "\n",
    "    return r\n",
    "    \n",
    "\n",
    "def calculate_rewards(prompts, completions, alpha: float = 0.8, **kwargs):\n",
    "    chunk = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for response in responses:\n",
    "        # First calculate r_len\n",
    "        r_len = get_length_reward(chunk, response)\n",
    "\n",
    "        # Now reconstruct input\n",
    "        reconstructed = recontruct_input(response)\n",
    "\n",
    "        r_bleu = get_bleu_score(chunk ,reconstructed)\n",
    "\n",
    "        r_final = alpha * r_bleu + (1-alpha) * r_len\n",
    "\n",
    "        rewards.append(r_final)\n",
    "\n",
    "    return rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e390e89",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80823dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = 1024 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0,\n",
    "    top_p = 0.8,\n",
    "    top_k = 20,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 0.7,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.001,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 1500,\n",
    "    save_steps = 300,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"/hpc/home/bfa6/work/github/yapper/results/test\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    fp16_full_eval = True,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    eval_accumulation_steps = 1,\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 15,131 | Num Epochs = 1 | Total steps = 1,500\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 20,185,088 of 616,235,008 (3.28% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-13 18:13:35 [processor.py:215] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        calculate_rewards\n",
    "    ],\n",
    "    args = training_args,\n",
    "    # train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    train_dataset = trainset,\n",
    "    eval_dataset = evalset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
